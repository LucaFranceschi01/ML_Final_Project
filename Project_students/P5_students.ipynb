{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"hnMagog7C_x-"},"source":["# <center> Project: **Customer Intelligence** department in a Bank company: real world examples of a **Data Scientist** in a Bank company. Part I: customer segmentation and loan/credit prediction\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0Hzjopj0C_yB"},"source":["# Project goals:\n","In this project, we are going to develop and apply different unsupervised and supervised Machine Learning techniques we have learnt during this ML course. This project has several objectives in order to introduce the student in real world use cases as a future Data Scientist.\n","\n","We work in the Customer Intelligence area of a bank company as a Data Scientist. In the financial sector (but also in general in any company) fraud detection and customer credit score are key in order to determine the risk before granting a loan. Complementary, Bank companies uses to qualify the asset (e.g. a house, a vehicle, etc...) that the customer pretends to buy in order to evaluate the risk that the credit cannot be payed back. \n","\n","Therefore, as a Customer Intelligence team member, you will be responsible for designing, developing and analyzing the **intelligence** to lead the business of our Bank company.\n","\n","\n","In particular:\n","\n","- You will apply unsupervised learning to cluster a customer base in order to \"understand\" the main patters and characteritics of the **groups** or **segments**. Customer segmentation is a very useful tool and crucial in any **data-driven** company. \n","\n","- You will also apply supervised learning to develop a model able to classify customers between high and low risk of default in case of receiving a credit or loan. \n","\n","- You will develop a regression model in order to determine an objective price for second hand vehicles, due to they are one of main reasons because our customers request a credit\n","\n","- As a bonus track we will complement the previous model as a classification stage that split between trucks (usually for professional customers) and cars (usually for particular customers) based on images.\n","\n","\n","To solve all these questions we will follow a common framework or way-of-working in Machine Learning projects: the **Machine Learning Operations (MLOps) life-cycle**. This framework is a common procedure in order to guarantee all stages in end-to-end Machine Learning project are covered: from the business problem understanding until to operation and maintenance of a solution.\n","\n","<img src='https://drive.google.com/uc?id=1EG0doe2ryshTGqoD5IsAJqZtOppDHNVT'>\n","\n","\n","\n","source: https://towardsdatascience.com/a-beginner-friendly-introduction-to-mlops-95282f25325c#aabc"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ThyIMxSgC_yC"},"source":["### Due date: up to xxx. \n","### Submission procedure: via Moodle."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ik6c2Cc7PKdw"},"source":["*******"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hC549QZYPDpk"},"source":["# **Part 0: Introduction to MLOPS**"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YTzCqvw8PZB3"},"source":["In the past, one of the main reason because the Machine Learning project failed was due to the lack of a robust and end-to-end procedure that covers all key stages of a project: from the design to maintenance and evolution of the solution.\n","Today we can find several definitions of MLOps but some of the most common are:\n","\n","(1) \"MLOps is a paradigm, including aspects like best practices, sets of concepts, as well as a development culture when it comes to the end-to-end conceptualization, implementation, monitoring, deployment, and scalability of machine learning products\" [Kreuzberger, D., Kühl, N., &Hirschl, S. Machine learning operations (mlops): Overview, definition, and architecture, 2022. doi:10.48550.arXiv preprint arXiv.2205.02302]\n","\n","(2) \"We can use the definition of Machine Learning Engineering (MLE), where MLE is the use of scientific principle, tools, and techniques of machine learning and traditional software engineering to design and build complex computing systems. MLE encompasses all stages from data collection, to model building, to make the model available for use by the product ot the consumers.\" (by A. Burkov) [https://ml-ops.org/content/motivation#mlops-definition]\n","\n","MLOps life-cycle consists mainly in three steps:\n","- **Design process**, that involves the definition of the use case problem and the main requirements in terms of production and maintenance.\n","- **Model development**, that includes all the data and model engineering\n","- **Operations process**, includes model deployment, monitoring and maintenance.\n","\n","This MLOps life-cycle follows a workflow or framework that specifies the concrete activities that take part of it:\n","\n","(1) **Business problem**: In any ML project is crucial to define the business problem or use case. A wrong definition will imply a failure in any of the next stages. To address this part of the workflow there are different several tools and ML canvas that facilitates the high-level description and main aspects of the system. An example of ML canvas could be:\n","\n","\n","<img src='https://drive.google.com/uc?id=1HzSlvc4w4wYXSJp1-OPy2mBt0LDjHpYV'>\n","\n","(2) **Data engineering or Data wrangling**: It consists on all data process ,management: from data gathering or ingestion until data understanding and preparation. This stage uses to require more than 50% of the human resources and it is crucial for the modelling stage. \n","- (a) data ingestion or gathering: implies to access IT systems to get the data sources and creating a dictionary to describe the variables that are part of these data sources.\n","- (b) **Exploratory Data Analysis**: implies a statistics analysis of the data including the usage of several visualization techniques as correlation matrix, boxplots, outliers identification, etc.... Data understanding will facilitate the identification of the most relevant data to our purpose.\n","- (c) **Data cleaning and preparation**: removing outliers, null management, categorical variables encoding,... are examples of main activities included in this sub-stage.\n","\n","(3) **Modelling or ML Model Engineering**: it includes model training, evaluation, testing and insights generation. As an output of this stage of the workflow, the ML model is packaged as a final step before been deployed in our ML infrastructure.\n","- (a) Model training implies the selection of the technique or combination of techniques that suits better for the use case. Feature engineering is also included.\n","- (b) Model evaluation and test: allows to determine the perfomance of the trained model and decide if it is good enough to our use case.\n","- (c) insight generation: Once the model is trained and validated its performance, in this sub-stage we go back to our initial stage (i.e. business problem) to ensure that it meets the business objectives defined as use case. \n","- (d) Model packing: Once the built ML has been validated and tested, the model is ready to be exported to the infrastructure responsible for executing, monitoring and maintenance.\n","\n","(4) **Code engineering**: in this final stage of the MLOps workflow the model is deployed into production where performance monitoring and logging is done. The subtasks are:\n","- (a) Model serving: it refers to how the model is integration of the final application or software. This integration could be done via API, on-demand serving, pre-calculated, etc.... The deployment of the model could be via a docker container in cloud or local or as a serveless function.\n","- (b) Model monitoring and logging: it refers to the periodic observation of the ML performance and comparision with original trained one. In case of large deviation, this sub-stage will generate an alarm or warning previous to the returning to previous stages to re-train the model. The performance of the model are saved in a log record to be analyzed.\n","\n","\n","In this project, we will focus on the **business problem**, **data engineering** and **data modelling** stages of the MLOps workflow:\n","\n","<img src='https://drive.google.com/uc?id=1HgG4ROiY5eqIVlNinaa21HoshLQZZWIq'>\n","\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cn_5QIADC_yD"},"source":["*******"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"6xjR-88IKZJQ"},"source":["# **Part I: Customer segmentation and load prediction**\n","In this first part of the project, we will apply unsupervised learning to cluster the Bank's customer base. We will learn how to apply the clustering using Python and how they are used to generate insights about our customer base, i.e. identify the main types or **sterotypes** of customers and their differences. Besides, we will learn to calculate the optimal K value and measure the quality of the clustering."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"euwcRZfAC_yE"},"source":["## Step 0. Understanding the problem: customers' stereotypes "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"LMZkBnEjC_yF"},"source":["As a data scientist in the **Customer Intelligence** department of a Bank company, we are responsible for identified the main **patterns** or **stereotypes** of our customer base. These **stereotypes** can be used for several purposes: from marketing campaigns to bank operations as acceptance or deney of credits or loans.\n"," \n","\n","To develop this customer segmenation, we are going **to apply unsupervised learning** and more specifically the two of the most important clustering techniques: K-means and Mixture of Gaussians (MoG).**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vMD1CqboC_yG"},"source":["# Step 1: Data gathering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"WBCPMk53C_yH"},"source":["\n","In this practice we are using a new dataset named `loan_prediction.csv`. This file contains information of **613 of our bank's consumers** that were accepted or denied to receive a loan in the past. In particular, the detailed information for each customer is:\n","\n","- *Loan_ID*: It's an integer that identifies any cuatomer.\n","- *Gender*: Male or female\n","- *Married*: Yes or No\n","- *Dependents*: Number of people that depends on the Loan_ID\n","- *Education*: Level of education (graduate or not-graduate) of the Loan_ID\n","- *Self_Employed*: Yes or No\n","- *ApplicantIncome*: Monthly income (€) of the Loan_ID\n","- *CoapplicantIncome*: Monthly income (€) of the Loan_ID's coapplicant in case of existing\n","- *LoanAmount*: Monthly quantity (€) of the loan\n","- *Loan_Amount_Term*: Duration of the loan\n","- *Credit_History*: It takes value 1 if the loan_id requested a loan in the past and 0 if he/she didn't\n","- *Property_Area*: Type of location of the property: Urban, Semiurban or Rural\n","- *Loan_Status*: Yes or No and it refers that the loan request was accepted or denied."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lD22OaRsC_yN"},"source":["# Step 2: Data understanding and preparation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IZ6q20FUC_yO"},"source":["Once we know the problem to solve, the next stage is to have a clear understanding of the data we have extracted and to prepare it before modelling. In particular, we will:\n","- List and verify the type of each variable (object, float, int...). Identify variables with nulls. Measure the memory usage\n","- Eliminate rows with nulls in order to have a dataset 100% fulfilled\n","- Aggregate rows with monthly expense per customers in order to have just 1 sample per customers\n","- Exploratory Data Analysis to understand main statistics (mean, standard deviation, min&max values and 25%-50%-75% quartiles) and distribution of the most relevant variables or features as data usage, voice usage, monthly expense and number of lines\n","- Plot several graphs in order to identify how variables are related between them. In particular:\n","- correlation matrix\n","- 2D and 3D scatter plots between data usage, voice usage and monthly expense\n","\n","Once this part, also known as **data wrangling** of the Project is done, we should achieve a deep knowledge about the data. Besides, the dataset will have been processed to be ready to apply the clustering algorithms to solve the business problem."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"0l3XWW7T3GvV"},"source":["Let's import the main Python libraries required in our project."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"t94PRZOK3GvV"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#import matplotlib.animation as animation\n","\n","\n","#%matplotlib notebook\n","import matplotlib.cm as cm\n","import seaborn as sns\n","from matplotlib import pyplot\n","from mpl_toolkits import mplot3d\n","from scipy.stats import chi2_contingency\n","from sklearn.metrics import pairwise_distances_argmin\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.cluster import KMeans\n","from sklearn.metrics import silhouette_samples, silhouette_score\n","from sklearn.mixture import GaussianMixture\n","from matplotlib.patches import Ellipse\n","\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","from graphviz import Source\n","from sklearn import tree\n","\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, AdaBoostClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, auc, roc_curve, classification_report, confusion_matrix, precision_score, recall_score, precision_recall_curve\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_5lfYuTUC_yQ"},"source":["**[EX0]** Open the csv with separator \",\" and assign to a dataframe variable (use read_csv from Pandas library). Let's see the top 5 elements."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(\"loan_prediction.csv\", sep=\",\")\n","display(df.head(5))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"o1rjGZecC_yW"},"source":["[**EX1**] Let's identify the type of the variables (integer, float, chart...) and the size of the dataset and the file. Which is the variable with more nulls? And with no nulls? \n","\n","Tip: [.info()](https://www.geeksforgeeks.org/python-pandas-dataframe-info/) is a function that reports the main characteristics of a dataframe."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: The variable with most nulls is Credit_History, as it has 50 nulls. The variables with no nulls (614 non-nulls of 614 entries) are: Index, Loan_ID, Education, ApplicantIncome, CoapplicantIncome, Property_Area and Loan_Status.</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qU8i852UC_ya"},"source":["We should guarantee that our dataset for training the cluster has no **nulls** in those variables.  "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_rz7xIchC_yb"},"source":["[**EX2**] Eliminate those rows with nulls in any of variables. We will use this new dataset from now for the rest of the project.\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"eT1O_XmsC_ye"},"source":["Let's re-calculate the type of the variables (integer, float, chart...) and the size of the dataset and the file. Your output should look like this:"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["customer_dt = df.dropna()       #not sure?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":444,"status":"ok","timestamp":1685335734286,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"ZW-UMuBVC_yf","outputId":"13e7b8ec-0d86-4207-a20e-ecb3814cc33e"},"outputs":[],"source":["customer_dt.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_-b0Xb3YC_yz"},"source":["In Machine Learning, it is key to understand the nature of the data before training. For numeric variables, it is useful to calculate the distribution and main statistics."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k-NHqupgC_y0"},"source":["[**EX3**] Calculate the main statistics (max, min, mean, median and standard deviation) of numerical variables. Plot a histogram for each of these variables\n","\n","Tip: use [Seaborn library](https://seaborn.pydata.org/) with `kde=True` to create a histogram. You also can use **dataframe_column.histplot(bins=number_of_bins)**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_statistics(df, column):\n","    max = df[column].max()\n","    min = df[column].min()\n","    mean = df[column].mean()\n","    median = df[column].median()\n","    std = df[column].std()\n","    return max, min, mean, median, std"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["columns = [\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\", \"Loan_Amount_Term\", \"Credit_History\"]\n","\n","fig, axs = plt.subplots(2, 3, sharex=False, sharey=False, figsize=(15, 10))\n","fig.delaxes(axs[1, 2])\n","fig.tight_layout()\n","\n","for i in range(0, len(columns)):\n","    max, min, mean, median, std = calculate_statistics(customer_dt, columns[i])\n","    print(\"%-17s ----> Max: %-10.3f Min: %-10.4f Mean: %-10.4f Median: %-10.4f Std: %.4f\\n\" % (columns[i], max, min, mean, median, std))\n","    sns.histplot(customer_dt[columns[i]], kde=True, ax=axs[int(i/3)][i%3])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"lXx_VvdoOEez"},"source":["**[EX4]** Create a box plot for the `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`variables. Do you identify any outlier? Justify your answer.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: Yes, we can identify many outliers. For instance, in the first boxplot the median is at around 4000, and the Q3 might be at around 7000. Applying the 1.5IQR rule, all the points that are above 10000 (approx.) are considered outliers. The same happens with the other boxplots.</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3jMm8RXrC_y6"},"source":["Additionaly to understanding each individual variable, it is important to understand how they are related to each other. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(1, 3, sharex=True, sharey=False, figsize=(10, 5))\n","fig.tight_layout()\n","\n","axs[0].boxplot(customer_dt[columns[0]])\n","axs[1].boxplot(customer_dt[columns[1]])\n","axs[2].boxplot(customer_dt[columns[2]])\n","axs[0].set_xlabel('Applicant Income')\n","axs[1].set_xlabel('Coapplicant Income')\n","axs[2].set_xlabel('Loan Amount')\n","plt.xticks([1], [''])\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"64w-J0FPC_y9"},"source":["[**EX5**] Calculate and plot the correlation matrix between customer attributes (i.e. `ApplicantIncome`, `CoapplicantIncome`, `LoanAmount`, `Loan_Amount_Term` and `Credit_History`. \n","- Which are the variables with more and less absolute correlation with respect to the `ApplicantIncome` variable?\n","- Which are the top 2 variables with highest correlation between them?\n","-and lowest?\n","\n","Tip: use [pandas.DataFrame.corr](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html) to compute a correlation matrix, and [matplotlib.pyplot.matshow](https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.matshow.html) to show this graphically."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\">Answer:\n","- The cariables with more absolute correlation are LoanAmount and CoapplicantIncome. The ones with less absolute correlation are: Loan_Amount_Term and Credit_History.\n","- The top 2 variables with highest correlation between them are: ApplicantIncome and LoanAmount. The top 2 lowest are: CoapplicantIncome and Loan_Amount_Term</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z7IzTAJTC_zB"},"source":["Another option to analyze the relation 1-to-1 between 2 variables in through scatter plots. Let's simplify the original dataset and create a new `training_dt`dataset with only `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["customer_attr_dt = customer_dt[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']]\n","A = customer_attr_dt.corr()\n","display(A)\n","\n","pyplot.matshow(A)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_dt = customer_dt[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\n","training_dt.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XHAKVxRMC_zE"},"source":["[**EX6**] Visualize a scatter plot with `ApplicantIncome` vs `Loan_Amount` variables. Could you visually identify any cluster? How many?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: If we had to identify any clusters from this plot, we would probably define two of them: one pretty much vertical in the left of the plot, very wide but not very high, and another one that would be more \"undefined\" in the right that would contain more dispersed points.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(training_dt[\"ApplicantIncome\"], training_dt[\"LoanAmount\"])\n","plt.xlabel(\"Applicant Income\")\n","plt.ylabel(\"Loan Amount\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qEuiu4brGmTu"},"source":["[**EX7**] Visualize a scatter plot with `ApplicantIncome` vs `CoapplicantIncome` variables. Could you visually identify any cluster? How many?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: If we had to identify any clusters from this plot, we would probably define two of them: One vertical in the left and another horizontal in the bottom.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.scatter(training_dt[\"ApplicantIncome\"], training_dt[\"CoapplicantIncome\"])\n","plt.xlabel(\"Applicant Income\")\n","plt.ylabel(\"Co-applicant Income\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"sNO30tLwKNmM"},"source":["[**EX8**] Visualize a scatter plot with `ApplicantIncome` vs `CoapplicantIncome` variables which values are below 20000 and over 0 respectively. Could you visually identify any cluster? How many?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: If we had to identify any clusters from this plot, we would probably define three of them: one that looks pretty much like a gaussian distribution centered around 3000 ApplicantIncome and 2000 CoapplicantIncome, one in the top of that first cluster containing more disperse vertical datapoints, and one in the right of the first cluster containing more disperse horizontal points.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["x = training_dt[(training_dt[\"ApplicantIncome\"] > 0) * (training_dt[\"CoapplicantIncome\"] > 0) * (training_dt[\"ApplicantIncome\"] < 20000) * (training_dt[\"CoapplicantIncome\"] < 20000)]\n","\n","plt.scatter(x[\"ApplicantIncome\"], x[\"CoapplicantIncome\"])\n","plt.xlabel(\"Applicant Income below 20000 and over 0\")\n","plt.ylabel(\"Co-applicant Income below 20000 and over 0\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"EWtSMJ_aal7V"},"source":["**[EX9]** Which type of clustering technique will fit better to this dataset? Justify your answer."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: Probably the best clustering technique that we know for this dataset would be K-means since, by the looking of the dataset, K-means should not have any trouble finding the best-fitting clusters (does not have elongated or clusters with very different variances) </font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZAoUllbjC_zJ"},"source":["[**EX10**] To improve our understanding of the data, plot a 3D visualization between `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`.\n","- Could you visually identify any cluster? How many?\n","- Could you identify a cluster bigger than the others? Describe approximately it in terms of the values of these 3 variables\n","\n","\n","Tip: use [scatter3d](https://matplotlib.org/3.1.1/gallery/mplot3d/scatter3d.html) to create 3D scatter plots."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(15, 15))\n","ax = fig.add_subplot(111, projection='3d')\n","# We use the new dataframe called x, that doesn't have any outliers in ApplicantIncome and CoapplicantIncome\n","ax.scatter3D(x[\"ApplicantIncome\"], x[\"CoapplicantIncome\"], x[\"LoanAmount\"])\n","ax.set_xlabel(\"Applicant Income\")\n","ax.set_ylabel(\"Coapplicant Income\")\n","ax.set_zlabel(\"Loan Amount\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AFdoMlT4dGMi"},"source":["[**EX11**] Rotate the plot 2 times to visualize the plot from other perspectives. "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(15, 15))\n","ax = fig.add_subplot(111, projection='3d')\n","# We use the new dataframe called x, that doesn't have any outliers in ApplicantIncome and CoapplicantIncome\n","ax.scatter3D(x[\"ApplicantIncome\"], x[\"CoapplicantIncome\"], x[\"LoanAmount\"])\n","ax.set_xlabel(\"Applicant Income\")\n","ax.set_ylabel(\"Coapplicant Income\")\n","ax.set_zlabel(\"Loan Amount\")\n","ax.view_init(elev=45, azim=90)\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(15, 15))\n","ax = fig.add_subplot(111, projection='3d')\n","# We use the new dataframe called x, that doesn't have any outliers in ApplicantIncome and CoapplicantIncome\n","ax.scatter3D(x[\"ApplicantIncome\"], x[\"CoapplicantIncome\"], x[\"LoanAmount\"])\n","ax.set_xlabel(\"Applicant Income\")\n","ax.set_ylabel(\"Coapplicant Income\")\n","ax.set_zlabel(\"Loan Amount\")\n","ax.view_init(elev=45, azim=180)\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7DBB-a4sK6rm"},"source":["**[EX12]** Let's analysis the distribution of some categorical variables as: `gender`, `Marital Status`, `Education`, `Self-Employment` and `Loan_Status`\n",". Create a bar plot for these 4 variables."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gender = customer_dt[\"Gender\"].value_counts()\n","married = customer_dt[\"Married\"].value_counts()\n","education = customer_dt[\"Education\"].value_counts()\n","self_employed = customer_dt[\"Self_Employed\"].value_counts()\n","loan_status = customer_dt[\"Loan_Status\"].value_counts()\n","\n","fig, axs = plt.subplots(2, 3, sharex=False, sharey=False, figsize=(15, 10))\n","fig.delaxes(axs[1, 2])\n","fig.tight_layout()\n","\n","variables = [gender, married, education, self_employed, loan_status]\n","variables_str = ['Gender', 'Married', 'Education', 'Self_employed', 'Loan_status']\n","\n","for i, name in zip(range(0, len(variables)), variables_str):\n","    ax=axs[int(i/3)][i%3]\n","    ax.bar(variables[i].index, variables[i].values)\n","    ax.set_title(name)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KwFlx-vBC_zP"},"source":["# Step 3-1: Training the model and performance evaluation: Segmentation of customers through K-means clustering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KYkcTTa4C_zQ"},"source":["Once the dataset has been processed and we have a first understanding of the type and characteristics of the variables, we are ready to apply clustering methods to group.\n","Firstly, we will code our own Kmeans algorithm. We will select `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount` variables to fit the clusters.\n","Once the clustering is done, we need to understand the output. 2-dimension and 3-dimension scatter plot visualizations are excellent techniques to evaluate the clustering output.\n","To check if our Kmeans algorithm works properly, we will use the Sklearn’s Kmeans function to cluster the dataset. We will compare the 2D and 3D plots from the Sklearn clustering and ours.\n","Finally, as part of any Machine Learning Project, we need to calculate the perfomance of our model. For Kmeans, we will 1) estimate the optimal K value through the Elbow method and 2) calculate the sihouette score for several values of K\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"3Cdo7H1eC_zQ"},"source":["## Your own K-means function"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1-1zZr5_C_zR"},"source":["[**EX13**] Build a `calculate_distance` function to calculate the distance between each point and the centroid"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"96V_hgOOC_zS"},"outputs":[],"source":["#Solution\n","def calculate_distance(X, centroids): # n_clusters x D\n","    squareDistance=np.zeros((X.shape[0], centroids.shape[0])) # squaredistance is N x n_clusters\n","\n","    for i in range(X.shape[0]):                \n","        for j in range(centroids.shape[0]):\n","            squareDistance[i][j] = np.sqrt(np.sum(np.power(X[i] - centroids[j], 2), axis=0))\n","\n","    return np.sqrt(squareDistance)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Qi1JkKiPC_zU"},"source":["[**EX14**] Build `K_means_clustering` function that creates a clustering according to K-means methodology."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QMOpImtiC_zU"},"outputs":[],"source":["#Solution\n","def K_means_clustering(X, n_clusters=2, seed=1, num_iterations=10):\n","    # Initialitize centroids based on a random selection of #n_clusters samples of X \n","    rng = np.random.RandomState(seed)\n","    i = rng.permutation(X.shape[0])[:n_clusters]\n","    centroids = X[i]\n","    # labels = np.zeros(X.shape[0])\n","    new_centroids = np.zeros((n_clusters, X.shape[1])) # n_clusters x D\n","    \n","    # Repeat the process during num_iterations or convergence achieved\n","    for num in range(0, num_iterations):\n","        # For each iteration, calculate the shortest distance of each point of X to centroids\n","        distances = calculate_distance(X, centroids)            \n","        labels = np.argmin(distances, axis=1)\n","\n","        #Calculate the new centroids based on the means of each point assigned to each cluster \n","        for i in range(n_clusters):\n","            new_centroids[i] = np.array(X[labels==i].mean(axis=0))\n","\n","    # Evaluate convergence: if new_centroids=centroids, stop iterations\n","        if np.all(centroids == new_centroids):\n","            print('Convergence achieved with:', num, 'iterations')\n","            break\n","        else:\n","            if num%10 == 0 and num != 0:\n","                print('No convergence yet after', num, 'iterations')\n","        centroids = new_centroids\n","        \n","    return centroids, labels"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"weuHT--kC_zX"},"source":["Let's define the `training_dt` dataset based on the following variables: `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_kK5RkQMC_zY"},"outputs":[],"source":["_training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"rIsXrFy6C_zb"},"source":["[**EX15**]Apply log transformation (np.log()) to `ApplicantIncome` and `LoanAmount` variables and standarize (StandardScaler()) all 3 variables (i.e. `ApplicantIncome`,`LoanAmount` and `CoapplicantIncome`) and store all these transformations to a new `training_dt` that will be the dataframe to use in all Clustering exercises. Execute your `K_means_clustering` function to this new `training_dt` and number of clusters=3. Calculate the centroids of each cluster."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_training_dt.loc[:, 'ApplicantIncome'] = np.log(_training_dt.loc[:, 'ApplicantIncome'])\n","_training_dt.loc[:, 'LoanAmount'] = np.log(_training_dt.loc[:, 'LoanAmount'])\n","\n","scaler = StandardScaler()\n","scaler.fit(_training_dt)\n","training_dt = scaler.transform(_training_dt)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["centroids, cluster_id = K_means_clustering(training_dt, n_clusters=3)\n","\n","training_dt = scaler.inverse_transform(training_dt)\n","centroids = scaler.inverse_transform(centroids)\n","\n","training_dt[:, 0] = np.exp(training_dt[:, 0])\n","training_dt[:, 2] = np.exp(training_dt[:, 2])\n","\n","centroids[:, 0] = np.exp(centroids[:, 0])\n","centroids[:, 2] = np.exp(centroids[:, 2])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Y0LnAvJHC_zd"},"source":["Now, it's time to understand how the clustering process works! To do it, we are plotting the `training_dt` painting the colour based on `Cluster_id`, output from the k-means. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k8jWJPNmC_ze"},"source":[" [**EX16**] Plot the following scatter plots representing the centroids:\n"," - Between `ApplicantIncome` vs `CoapplicantIncome`\n"," - Between `ApplicantIncome`vs `LoanAmount` and\n"," - Between `CoapplicantIncome`vs `LoanAmount`\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(2, 2, sharex=False, sharey=False, figsize=(15, 12))\n","fig.delaxes(axs[1, 1])\n","\n","axs[0][0].scatter(training_dt[:, 0], training_dt[:, 2], c=cluster_id)\n","axs[0][0].scatter(centroids[:, 0], centroids[:, 2], c='red', marker='x')\n","axs[0][0].set_xlabel(\"Applicant Income\")\n","axs[0][0].set_ylabel(\"Loan Amount\")\n","\n","axs[0][1].scatter(training_dt[:, 1], training_dt[:, 2], c=cluster_id)\n","axs[0][1].scatter(centroids[:, 1], centroids[:, 2], c='red', marker='x')\n","axs[0][1].set_xlabel(\"Coapplicant Income\")\n","axs[0][1].set_ylabel(\"Loan Amount\")\n","\n","axs[1][0].scatter(training_dt[:, 0], training_dt[:, 1], c=cluster_id)\n","axs[1][0].scatter(centroids[:, 0], centroids[:, 1], c='red', marker='x')\n","axs[1][0].set_xlabel(\"Applicant Income\")\n","axs[1][0].set_ylabel(\"Coapplicant Income\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"k_rhWzA4JCtz"},"source":["**[EX17]** According to these scatter plots, would you change the value of K? Which one and why?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: Based on these scatter plots, we would keep the K=3 since it seems a reasonable clustering count, all of the scatter plots makes sense in its way and it does not seem to \"overfit\" with unnecesary clusters</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"zVF3eNuhC_zi"},"source":["[**EX18**] Execute the Sklearn library's KMeans function and compare both `ApplicantIncome` vs `LoanAmount`scatter plots. Are they similar?\n","\n","Tip: We recommend the following  [KMeans()](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) parameters: `init`='random', `n_init`=10, `tol`=1e-04 and `random_state`=0"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["_training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount']]\n","_training_dt.loc[:, 'ApplicantIncome'] = np.log(_training_dt.loc[:, 'ApplicantIncome'])\n","_training_dt.loc[:, 'LoanAmount'] = np.log(_training_dt.loc[:, 'LoanAmount'])\n","\n","scaler = StandardScaler()\n","scaler.fit(_training_dt)\n","training_dt = scaler.transform(_training_dt)\n","\n","kmeans = KMeans(n_clusters=3, init='random', n_init=10, tol=1e-04, random_state=0).fit(_training_dt)\n","\n","kmeans_c = kmeans.cluster_centers_\n","kmeans_l = kmeans.labels_\n","\n","training_dt = scaler.inverse_transform(training_dt)\n","\n","training_dt[:, 0] = np.exp(training_dt[:, 0])\n","training_dt[:, 2] = np.exp(training_dt[:, 2])\n","\n","kmeans_c[:, 0] = np.exp(kmeans_c[:, 0])\n","kmeans_c[:, 2] = np.exp(kmeans_c[:, 2])\n","\n","print(kmeans_c)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(3, 2, sharex=False, sharey=False, figsize=(15, 15))\n","\n","axs[0][0].set_title(\"Sklearn KMeans\")\n","axs[0][1].set_title(\"Homemade K_means_clustering\")\n","\n","axs[0][0].scatter(training_dt[:, 0], training_dt[:, 1], c=kmeans_l)\n","axs[0][0].scatter(kmeans_c[:,0], kmeans_c[:, 1], c='red', marker='x')\n","axs[0][0].set_xlabel(\"Applicant Income\")\n","axs[0][0].set_ylabel(\"Coapplicant Income\")\n","\n","axs[0][1].scatter(training_dt[:, 0], training_dt[:, 1], c=cluster_id)\n","axs[0][1].scatter(centroids[:,0], centroids[:,1], c='red', marker='x')\n","axs[0][1].set_xlabel(\"Applicant Income\")\n","axs[0][1].set_ylabel(\"Coapplicant Income\")\n","\n","axs[1][0].scatter(training_dt[:, 0], training_dt[:, 2], c=kmeans_l)\n","axs[1][0].scatter(kmeans_c[:,0], kmeans_c[:, 2], c='red', marker='x')\n","axs[1][0].set_xlabel(\"Applicant Income\")\n","axs[1][0].set_ylabel(\"Loan Amount\")\n","\n","axs[1][1].scatter(training_dt[:, 0], training_dt[:, 2], c=cluster_id)\n","axs[1][1].scatter(centroids[:,0], centroids[:,2], c='red', marker='x')\n","axs[1][1].set_xlabel(\"Applicant Income\")\n","axs[1][1].set_ylabel(\"Loan Amount\")\n","\n","axs[2][0].scatter(training_dt[:, 1], training_dt[:, 2], c=kmeans_l)\n","axs[2][0].scatter(kmeans_c[:,1], kmeans_c[:, 2], c='red', marker='x')\n","axs[2][0].set_xlabel(\"Coapplicant Income\")\n","axs[2][0].set_ylabel(\"Loan Amount\")\n","\n","axs[2][1].scatter(training_dt[:, 1], training_dt[:, 2], c=cluster_id)\n","axs[2][1].scatter(centroids[:,1], centroids[:,2], c='red', marker='x')\n","axs[2][1].set_xlabel(\"Coapplicant Income\")\n","axs[2][1].set_ylabel(\"Loan Amount\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"xBDuranWC_zo"},"source":["## Measuring the quality of the clustering and the optimal K: Elbow method and sihouette"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HYHKv-2uC_zp"},"source":["The number of clusters to choose may not always be so obvious in real-world applications, especially if we are working with a high dimensional dataset that cannot be visualized."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GwqvSmUqC_zs"},"source":["The elbow method is a useful graphical tool to estimate the optimal number of clusters. Intuitively, we can say that, if k increases, the distorsion within each cluster will decrease because the samples will be closer to their centroids. However, sometimes is not efficient to increase the **K** value because the distorsion doesn't decrease enough in comparision with the computation load required for higher **K**. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"z65jtHdFC_zt"},"source":["**[EX 19]** Let's calculate the Elbow method for the previous dataset, i.e. containing only `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount` variables for K values from 1 to 10.\n","We use [km.inertia_](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) from the Sklearn library's KMeans to measure the sum of squared distances of samples to their closest cluster center. Which is the optimal value for K?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: Following the Elbow Method the optimal K value should be 3, since it's the value for which the decrease in MSE is high but the value of K is not exagerated.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yfyqVkpTFAp-"},"outputs":[],"source":["# Selection of the dataset\n","training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome','LoanAmount']]\n","inertia = []\n","#Calculate the Kmeans from K=1 to 10\n","for i in range(1, 11):\n","    km = KMeans(\n","        n_clusters=i, init='random',\n","        n_init=10, max_iter=10,\n","        tol=1e-04, random_state=0\n","    )\n","    km.fit(training_dt)\n","    inertia.append(km.inertia_)\n","\n","\n","\n","# plot inertia\n","plt.title('Elbow method')\n","plt.xlabel('K')\n","plt.ylabel('MSE')\n","plt.xticks(np.arange(10), np.arange(1, 11, 1).tolist())\n","plt.plot(inertia, 'o-')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XbIgiCenC_z1"},"source":["**Silhouette** is a metric to measure the *quality* of the clustering process. Clustering models with a high **Silhouette** are said to be dense, i.e. samples in the same cluster are similar to each other, and well separated, where samples in different clusters are not very similar to each other. This measure has a range of [-1, 1]."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kotDCWoaC_z2"},"source":["[**EX20**]Calculate the `silhouette_score` value for a range of KMeans clusters from 2 to 7. The dataset to use is `training_dt` with the following variables: `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`. Which is the value of **K** with better **Silhouette**? Does it make sense taking into consideration the previous scatter plots?\n","\n","Tip: use [silhouette_score](https://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) to calculate the silhouette score and further information."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: The best silhouette score is achieved with K=2 (0.76). Since the previous scatter plots are made with K=3, it does not make sense taking into consideration those.</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":391,"status":"ok","timestamp":1685335747484,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"b5Z794FlC_z2","outputId":"07c1a6b4-e339-4001-b4df-d39a9380ffb9"},"outputs":[],"source":["#Solution\n","# Selection of the dataset\n","training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome','LoanAmount']]\n","for j in range(2, 8):\n","    \n","    km = KMeans(n_clusters=j, init='random', n_init=10, max_iter=10, tol=1e-04, random_state=0)\n","    km.fit(training_dt)\n","\n","    cluster_labels = km.labels_\n","\n","    silhouette_avg = silhouette_score(training_dt, cluster_labels)\n","    print(\"For n_clusters =\", j,\n","          \"The average silhouette_score is :\", silhouette_avg)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bEtT8JqCC_z5"},"source":["For a visual understanding about each cluster, we can plot the silhouette score for each sample of the dataset. Execute the following code:"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":2380,"status":"ok","timestamp":1685335753199,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"LLU_yMj-C_z6","outputId":"a1cf594f-6513-4c44-9c11-63b9cc506ba0"},"outputs":[],"source":["training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome','LoanAmount']]\n","for j in range(2, 8):\n","    n_clusters=j\n","    km  = KMeans(j, random_state=10, n_init=10)\n","    cluster_labels = km.fit_predict(training_dt)\n","    silhouette_avg = silhouette_score(training_dt, cluster_labels)\n","    \n","    # Compute the silhouette scores for each sample\n","    sample_silhouette_values = silhouette_samples(training_dt, cluster_labels)\n","    # Create a subplot with 1 row and 1 columns\n","    fig, (ax1) = plt.subplots(1,1)\n","    fig.set_size_inches(8, 5)\n","\n","    # The 1st subplot is the silhouette plot\n","    # The silhouette coefficient can range from -1, 1 but in this example all\n","    # lie within [-0.1, 1]\n","    ax1.set_xlim([-0.1, 1])\n","    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n","    # plots of individual clusters, to demarcate them clearly.\n","    ax1.set_ylim([0, len(training_dt) + (n_clusters + 1) * 10])\n","    \n","    y_lower = 10\n","    for i in range(j):\n","        # Aggregate the silhouette scores for samples belonging to\n","        # cluster i, and sort them\n","        ith_cluster_silhouette_values = \\\n","            sample_silhouette_values[cluster_labels == i]\n","\n","        ith_cluster_silhouette_values.sort()\n","\n","        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n","        y_upper = y_lower + size_cluster_i\n","\n","        color = cm.nipy_spectral(float(i) / n_clusters)\n","        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n","                          0, ith_cluster_silhouette_values,\n","                          facecolor=color, edgecolor=color, alpha=0.7)\n","\n","        # Label the silhouette plots with their cluster numbers at the middle\n","        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n","\n","        # Compute the new y_lower for next plot\n","        y_lower = y_upper + 10  # 10 for the 0 samples\n"," \n","    ax1.set_title(\"The silhouette plot for the various clusters\")\n","    ax1.set_xlabel(\"The silhouette coefficient values\")\n","    ax1.set_ylabel(\"Cluster label\")\n","\n","    # The vertical line for average silhouette score of all the values\n","    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n","\n","    ax1.set_yticks([])  # Clear the yaxis labels / ticks\n","    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"HFCIlp3_C_z8"},"source":["# Step 4: Insights generation: Understanding the clustering output"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QUG59j31C_z9"},"source":["Let's consider that **K=2** is good enough to cluster our customer base and generate insights for the Bank company."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"QU5nmjfyC_z9"},"source":["[**EX21**]Repeat the K-Means clustering with **K=2** for the `training_dt` formed by `ApplicantIncome`, `CoapplicantIncome` and `LoanAmount`. Later, apply the inverse of standardscaler to all 3 variables and finally apply the inverse of np.log (i.e. np.exp) to `ApplicantIncome` and `LoanAmount` variables. For each cluster, calculate the **mean**, **standard deviation**, **min**, **max** for each variable.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def calculate_cluster_stats(X, labels, centroids):\n","    columns = [\"ApplicantIncome\", \"CoapplicantIncome\", \"LoanAmount\"]\n","    for i in range(0, len(centroids)):\n","        points_cluster = X[labels==i]\n","        for j in range(0, 3):\n","            max = points_cluster[j].max()\n","            min = points_cluster[j].min()\n","            mean = points_cluster[j].mean()\n","            std = points_cluster[j].std()\n","            print(\"Cluster %-4s %-17s ----> \\tMax: %-10.3f Min: %-10.4f Mean: %-10.4f Std: %.4f\\n\" % (i, columns[j], max, min, mean, std))\n","        print(\"\\n\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["training_dt=customer_dt[['ApplicantIncome', 'CoapplicantIncome','LoanAmount']]\n","training_dt.loc[:, 'ApplicantIncome'] = np.log(training_dt.loc[:, 'ApplicantIncome'])\n","training_dt.loc[:, 'LoanAmount'] = np.log(training_dt.loc[:, 'LoanAmount'])\n","\n","scaler = StandardScaler()\n","scaler.fit(training_dt)\n","training_dt = scaler.transform(training_dt)\n","\n","kmeans = KMeans(n_clusters=2, init='random', n_init=10, tol=1e-04, random_state=0).fit(_training_dt)\n","\n","training_dt = scaler.inverse_transform(training_dt)\n","\n","training_dt[:, 0] = np.exp(training_dt[:, 0])\n","training_dt[:, 2] = np.exp(training_dt[:, 2])\n","\n","calculate_cluster_stats(training_dt, kmeans.labels_, kmeans.cluster_centers_)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig, axs = plt.subplots(2, 2, sharex=False, sharey=False, figsize=(15, 12))\n","fig.delaxes(axs[1, 1])\n","\n","axs[0][0].scatter(training_dt[:, 0], training_dt[:, 2], c=kmeans.labels_)\n","axs[0][0].scatter(np.exp(kmeans.cluster_centers_[:, 0]), np.exp(kmeans.cluster_centers_[:, 2]), c='red', marker='x')\n","axs[0][0].set_xlabel(\"Applicant Income\")\n","axs[0][0].set_ylabel(\"Loan Amount\")\n","\n","axs[0][1].scatter(training_dt[:, 1], training_dt[:, 2], c=kmeans.labels_)\n","axs[0][1].scatter(kmeans.cluster_centers_[:, 1], np.exp(kmeans.cluster_centers_[:, 2]), c='red', marker='x')\n","axs[0][1].set_xlabel(\"Coapplicant Income\")\n","axs[0][1].set_ylabel(\"Loan Amount\")\n","\n","axs[1][0].scatter(training_dt[:, 0], training_dt[:, 1], c=kmeans.labels_)\n","axs[1][0].scatter(np.exp(kmeans.cluster_centers_[:, 0]), kmeans.cluster_centers_[:, 1], c='red', marker='x')\n","axs[1][0].set_xlabel(\"Applicant Income\")\n","axs[1][0].set_ylabel(\"Coapplicant Income\")\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"x_2iKOgklanL"},"source":["**[EX22]** Describe with one sentence the main characteristic of every customer segment in terms of this 3 variables?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: The first customer segment (cluster 0) has a higher coapplicant income with respect to applicant income and loan amount. The second customer segment (cluster 1) has a lower coapplicant income with respect to the other two variables. However, we can see that with both segments the Loan Amount with respect to Applicant Income seems to be non-related.</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"XKF9XfDWC_0A"},"source":["# Step 3-2: Training the model and performance evaluation: Segmentation of customers through Mixture of Gaussian clustering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9-8DfhgTC_0A"},"source":["As we know, there are other mechanisms to cluster a dataset. Let's test how Mixture of Gaussians function from sklearn library works."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CUiaYTTJC_0B"},"source":["[**EX23**] Execute the Mixture of Gaussians function (with number of components=3) to `training_dt` dataset with `ApplicantIncome`, `CoapplicantIncome`and `LoanAmount` variables. \n","- Which is the size of each cluster? \n","- Visualize the scatter plot between `ApplicantIncome` vs `LoanAmount`. Is it similar to the resulting from K-Means and K=3?\n","- Visualize the scatter plot between `ApplicantIncome` vs `CoapplicantIncome`. Is it similar to the resulting from K-Means and K=3?\n","- Visualize the scatter plot between `CoapplicantIncome` vs `LoanAmount`. Is it similar to the resulting from K-Means and K=3?\n","\n","Tip: You may use [GaussianMixture](https://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html) from Sklearn libray."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"FOzxAeJvC_0I"},"source":["[**EX24**] Evaluate the **Silhouette** metric for MoG with **number of components** from 2 to 7. "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Ahhg8fFLm-zj"},"source":["**[EX25]** Which is the number of cluster with the highest score? Which method is finally the best for our dataset?\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_ITrpPxkqhqk"},"source":["# Step 3-3: Training the model and performance evaluation: Classification of customers to be granted a loan"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"M_DM-tMfPjXW"},"source":[" Until now, the credit risk department of our Bank defined and applied the criteria to approve or deny a loan. However, this criteria is differently applied between their members that belong to the risk department. To solve this situation and to have and apply a common criteria, our Customer Intelligence area has been requested to design and implement an algorithm to classify between loan request to be accepted or denied."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"MC4q_x37SMAy"},"source":["**[EX26]** Convert categorical columns to numerical using one-hot encoding and drop `Loan_ID`column. You should obtain something similar to:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":504},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1685335767102,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"4BhzUaaISxlm","outputId":"1f7eaa4e-2673-4c0a-ada4-dad8a1d527d7"},"outputs":[],"source":["customer_dt_encoded"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1685335768931,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"HD7_JhXwT11v","outputId":"50cf0e41-e050-4649-a626-c685167d125a"},"outputs":[],"source":["customer_dt_encoded.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"kA0W19uITHYU"},"source":["**[EX27]** Split the data into: a) into features (X) and target(i.e. `Loan_Status`) (y) and b) training (80% of total dataset) and test sets (20% of total dataset)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yS2OIvyujRJv"},"source":["### 3.3.1 Baseline of models: Training and evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_xeX8v30jbJd"},"source":["[**EX28**] Train the Decision Tree algorithm from Sklearn library. Evaluate the following metrics:\n","- Which is **precision**, **recall** and **accuracy** of the algorithm?\n","- Which is the **confusion matrix**?\n","- Visualize the Decision Tree using tree.export_graphviz.\n","- Train a second Decision Tree with the following hyperparameters: `max_depth`=5, `min_samples_split`=5, `min_samples_leaf`=5, `random_state`=42. Calculate **precision**, **recall**, **accuracy** and the **confusion matrix**. Has the performance improved? "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"M47TVmu9jwYE"},"source":["[**EX29**] Train the Logistic Regression algorithm from Sklearn library. Evaluate the following metrics:\n","- Which is **precision**, **recall** and **accuracy** of the algorithm?\n","- Which is the **confusion matrix**?\n","- Is Logistic Regression algorithm working better than DT? Why?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Le707BCwj-Or"},"source":["[**EX30**] Scale the numerical columns using StandardScaler function. Train again the Logistic Regression algorithm from Sklearn library. Evaluate the following metrics:\n","- Which is **precision**, **recall** and **accuracy** of the algorithm?\n","- Which is the **confusion matrix**?\n","- Is Logistic Regression algorithm working better than the previous LR? Why?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dG4M2FeqaCzC"},"outputs":[],"source":["# Scale the numerical columns using StandardScaler\n","scaler = StandardScaler()\n","numerical_cols = ['ApplicantIncome', 'CoapplicantIncome', 'LoanAmount', 'Loan_Amount_Term', 'Credit_History']\n","X_train_scaled=X_train.copy()\n","X_test_scaled=X_test.copy()\n","X_train_scaled[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])\n","X_test_scaled[numerical_cols] = scaler.transform(X_test[numerical_cols])"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"_Ab9BoU0l-yH"},"source":["### Comparing algorithm consistenly: KFold cross-validation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"wbD4b_6zmBgK"},"source":["When we are looking for the best algorithm to classify a dataset, it is very useful to compare all of them. Besides, to protect the training from **overfitting** and calculate the performance with less variance than a single train-test split, it is uselful to apply **K-Fold cross-validation**. The way that KFolds works is splitting the dataset into k-parts or **fold** (e.g. k = 3, 5 or k = 10). The algorithm is trained on k − 1 folds with one held back and tested on the held back fold."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ib0-NX9eXM0F"},"source":["[**EX31**] Train a Decision Tree and Logistic Regression algorithms using a KFold cross-validation with **k=5** and calculate the **mean** and **standard deviation** of the **accuracy**. Plot a boxplot of the accuracy for every model. Which is the model with better mean value of the accuracy? Which is the algorithm with less deviation on the accuracy?  \n","\n","Tip 1: You may use [KFold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html) to apply cross-validation.\n","\n","Tip 2: You may use [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) to evaluate"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"opDodugrCnyD"},"outputs":[],"source":["#Models definition\n","models=[]\n","models.append(('LR', LogisticRegression(max_iter=1000)))\n","models.append(('Decision_trees', DecisionTreeClassifier()))\n","#Evaluate each models\n","results=[]\n","names=[]\n","scoring_metric='accuracy'\n","for name_model, model in models:\n","#Solution\n","\n","    \n","    results.append(cv_results)\n","    names.append(name_model)\n","    print (\"Model\", name_model, \"with accuracy (mean):\", cv_results.mean(), \"and accuracy (std):\", cv_results.std())\n","\n","\n","#boxplot for algorithm comparison\n","fig=pyplot.figure()\n","fig.suptitle (\"Algorithm accuracy comparison\")\n","ax=fig.add_subplot(111)\n","pyplot.boxplot(results)\n","ax.set_xticklabels(names)\n","pyplot.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hAwFNyyjkjQg"},"source":["### 3.3.2 Improving the model using ensembling models: voting, bagging and boosting"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"KUUO_WOdkzbq"},"source":["The three most popular methods for combining models are:\n","- Bagging combines multiple models that are trained with different subsamples of the training dataset.\n","- Boosting combines multiple models in cascade and each of them learns to fix the prediction errors of the prior model.\n","- Voting combines statistically the output of several models.\n","\n","Usually Bagging and Boosting are formed by models of the same type meanwhile voting could be formed by different models."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"bH1uf0DqlGcH"},"source":["### Voting ensemble"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"AdS_u6F3kQK3"},"source":["[**EX32**] Build a **voting** ensemble formed by a Logistic Regression and Decision Tree. Calculate the **precision**, **recall** and **confusion matrix** of the new classifier. Is it better than any of the previous baseline models? Justify your answer.\n","\n","Tip: You may use [VotingClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.VotingClassifier.html) to build this type of ensemble."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hh3cBqeCC2R-"},"outputs":[],"source":["# create the sub models\n","estimators = []\n","model1 = LogisticRegression(max_iter=1000)\n","estimators.append(('LR', model1))\n","model2 = DecisionTreeClassifier()\n","estimators.append(('DecisionTree', model2))\n","\n","# create the ensemble model\n","#Solution\n","\n","\n","\n","y_pred_ensemble=ensemble.predict(X_test)\n","print(\"*********************************** VOTING ENSEMBLE*****************************************\")\n","result_ensemble=ensemble.score(X_test, y_test)\n","print(\"Accuracy:\", result_ensemble)\n","matrix_ensemble=confusion_matrix(y_test, y_pred_ensemble)\n","print(\"Confusion matrix:\\n\", matrix_ensemble)\n","report_ensemble=classification_report(y_test, y_pred_ensemble)\n","print(report_ensemble)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SiRpKsp1lKII"},"source":["### Bagging ensemble: Random Forest"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RlN1WagplOj9"},"source":["[**EX33**] Build a **Bagging** ensemble based on Random Forest. Random Forest is considered a bagging ensemble formed by Decision Trees algorithms. Train the Random Forest with `X_train` and `y_train`. Calculate the **precision**, **recall** and **confusion matrix** of the new classifier. Is it better than any of the previous baseline models? Justify your answer.\n","\n","Tip: You may use [RandomForestClassifier()](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) to build this type of ensemble."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"hinmWQuPlWsk"},"source":["It is also important to evaluate the probabilities distribution of the prediction. Execute this code to plot the histograms of the probabilities resulting of the prediction of the Random Forest model for class 0 and class 1."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":1579,"status":"ok","timestamp":1685297967515,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"KCelNhWgXzYs","outputId":"0390c4f9-467e-48d6-d0a5-c46cf4057f04"},"outputs":[],"source":["\n","y_pred_proba_RF=model_RF.predict_proba(X_test)\n","\n","y_pred_total_RF=np.concatenate((y_pred_proba_RF[:,1].reshape(-1,1),np.asarray(y_test).reshape(-1,1)), axis=1)\n","y_test_1_RF=y_pred_total_RF[y_pred_total_RF[:,1]==\"Y\"]\n","y_test_0_RF=y_pred_total_RF[y_pred_total_RF[:,1]==\"N\"]\n","\n","sns.set(rc={'figure.figsize':(7,5)})\n","sns.histplot(y_test_1_RF[:,0],kde=True, bins=50, color=\"r\")\n","plt.title('Histogram of scores for samples Target=1 of X_test')\n","plt.xlabel('Score of the model')\n","plt.ylabel('Number of samples')\n","plt.show()\n","\n","sns.histplot(y_test_0_RF[:,0],kde=True, bins=50, color=\"b\")\n","plt.title('Histogram of scores for samples Target=0 of X_test')\n","plt.xlabel('Score of the model')\n","plt.ylabel('Number of samples')\n","plt.show()\n","\n","sns.set(rc={'figure.figsize':(7,5)})\n","sns.histplot(y_test_1_RF[:,0],kde=True, bins=50, color=\"r\")\n","plt.title('Histogram of scores for samples Target=1 and Target=0 of X_test')\n","sns.histplot(y_test_0_RF[:,0],kde=True, bins=50, color=\"b\")\n","plt.xlabel('Score of the model')\n","plt.ylabel('Number of samples')\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"n2DjCnBu7v6a"},"source":["**[EX34]** As the dataset has more samples for class \"Y\" than for class \"N\", the training process might be affected by the unbalanced scenario. Random Forest's `class_weight`=\"balanced\" will fix it. Train a new RF model including `class_weight`=\"balanced\". Has the RF's performance improved? Is the unbalanced class affecting the performace?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"X1TDXvzalmIv"},"source":["### Boosting ensemble: Gradient Tree Boosting"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"iVmWqBW8lpeS"},"source":["[**EX35**] Build a **Boosting** ensemble based on Gradient Tree Boosting (GBT). There are several boosting algorithms as Adaboost, etc.  Train the GBT with `X_train` and `y_train`. Calculate the **precision**, **recall** and **confusion matrix** of the new classifier. Is it better than any of the previous baseline models? Justify your answer.\n","\n","Tip: You may use [GradientBoostingClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingClassifier.html) to build this type of ensemble."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XQdJcOzwchEj"},"outputs":[],"source":[]}],"metadata":{"colab":{"collapsed_sections":["KwFlx-vBC_zP","3Cdo7H1eC_zQ","xBDuranWC_zo","HFCIlp3_C_z8","XKF9XfDWC_0A","xj1-ZEjOC_0N"],"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
