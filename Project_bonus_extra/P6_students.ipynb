{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{"id":"4AZpW1_1eqvs"},"source":["# <center> Project: **Customer Intelligence** department in a Bank company: real world examples of a **Data Scientist** in a Bank company. Part II: Extra bonus - Regression model for car price estimation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"DhEXMGODeqvu"},"source":["# Project goals:\n","In this part of the project, we continue working as a Data Scientist in our Bank company facing to new challenges in the area of Machine Learning. \n","\n","In particular, the Bank credit department is realizing that a lot of customers are asking credits to purchase second-hand vehicles. Until now they have used a price reference book but this last is not too accurate and the final consequence is the Bank is not measuring correctly the risk of granting these credits.\n","\n","Therefore, as a Customer Intelligence team member, you will be responsible for designing, developing and analyzing a model to estimate the price of a second hand vehicle based on its main characteristics. To do it we will use a **Multilayer Perceptron (MLP)** architecture."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"pRzeAGhqA8Wi"},"source":["### Due date: up to Junem 18th at 23:59h. \n","### Submission procedure: via Moodle."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YRTBF1prBEcp"},"source":["# Step 1: Data gathering"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Lk1KfnrFBFja"},"source":["In this part of the Project we are using a new dataset named `CarPrice.csv`. This file contains information of **205 of cars** and 26 features that describes the main characteristics of every car. Some examples of these ones are:\n","\n","- *car_ID*: It's an integer that identifies any car.\n","- *symboling*: It's an integer that identifies a car category\n","- *CarName*: Manufacture and model of the car\n","- *fueltype*: Type of fuel that the car uses \n","- ...\n","- *price*: The current price of the car in the market\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"gzYQ-ivxILao"},"source":["Let's upload some libraries and function we will need to develop our model."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":1827,"status":"ok","timestamp":1686544457573,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"OsbFuvmBeqvv"},"outputs":[],"source":["# Imports\n","import numpy as np \n","from matplotlib import pyplot as plt\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","#import matplotlib.animation as animation\n","\n","\n","#%matplotlib notebook\n","import matplotlib.cm as cm\n","import seaborn as sns\n","from matplotlib import pyplot\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import Normalizer\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import LabelEncoder\n","\n","from sklearn.model_selection import train_test_split, KFold, cross_val_score\n","from sklearn.metrics import r2_score, mean_absolute_error"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"vBI002-U4k1H"},"source":["**[EX0]** Upload the car price into a Dataframe named `car_price_dt`. You should obtain a dataframe similar to this one."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":279},"executionInfo":{"elapsed":354,"status":"ok","timestamp":1686542730896,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"-wIepbw6tTV4","outputId":"0ea5e93d-d277-482f-9d10-ccc4b398bfde"},"outputs":[],"source":["df = pd.read_csv(\"car_prediction.csv\", sep=\";\")\n","display(df.head())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"68A8Ld0eDgbu"},"source":["# Step 2: Data understanding and preparation"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9vFpFKJbDkOU"},"source":["Once we know the problem to solve, the next stage is to have a clear understanding of the data we have extracted and to prepare it before modelling. In particular, we will:\n","- List and verify the type of each variable (object, float, int...). Identify variables with nulls. Measure the memory usage\n","- Eliminate rows with nulls in order to have a dataset 100% fulfilled\n","- Exploratory Data Analysis to understand main statistics (mean, standard deviation, min&max values and 25%-50%-75% quartiles) and distribution of the most relevant variables or features\n","- Plot several graphs in order to identify how variables are related between them. In particular:\n","- correlation matrix\n","- 2D and 3D scatter plots\n","\n","Once this part, also known as **data wrangling** of the Project is done, we should achieve a deep knowledge about the data."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7rv5LhSBEEZU"},"source":["**[EX1]** Is there any null variable to fix?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Pjsa75CjEoCw"},"source":["**[EX2]** Calculate the quartiles, maximum and minimum values for numeric features"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["numeric_features = [\"symboling\", \"wheelbase\", \"carlength\", \"carwidth\", \"carheight\", \"curbweight\", \"enginesize\", \"boreratio\", \"stroke\", \"compressionratio\", \"horsepower\", \"peakrpm\", \"citympg\", \"highwaympg\", \"price\"]\n","df_numeric = df[numeric_features]\n","\n","df_numeric.quantile([0.25, 0.5, 0.75])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"%37s\" % \"Min values:\", \"%25s\" % \"Max values:\")\n","for i in range(0, len(numeric_features)):\n","    print(\"%-25s\" % numeric_features[i], \"%-25.2f\" % df_numeric[numeric_features[i]].min(), \"%-15.2f\" % df_numeric[numeric_features[i]].max())"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"5jOvJ7d2F-dw"},"source":["**[EX3]** Plot the distribution of the column `price`"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# TODOO"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"RyVAKj77GMYe"},"source":["**[EX4]** Plot the correlation matrix between numerical features\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["corr = df_numeric.corr()\n","display(corr)\n","\n","pyplot.matshow(corr)\n","\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"Na0yGKCoGhl_"},"source":["**[EX5]** Look at the correlation matrix results. Do you think a model to predict car price is feasible? Justify your answer."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<font color=\"red\"> Answer: TODO</font>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"CoVSHxtvLP30"},"source":["**[EX6]** Filter from the previous dataset (i.e. `car_price_dt`) the following columns: `symboling`, `wheelbase`, `carlength`, \n","             `carwidth`, `carheight`, `curbweight`, \n","             `enginesize`, `boreratio`, `stroke`, \n","             `compressionratio`, `horsepower`, `peakrpm`, \n","             `citympg`, `highwaympg`, `price`. Normalize with **StandardScaler()** function all these features. Finally, create two new arrays: `y` with the `price` column and `x` with the rest of features."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filtered_columns = ['symboling', 'wheelbase', 'carlength', 'carwidth', 'carheight', 'curbweight', 'enginesize', 'boreratio', 'stroke', 'compressionratio', 'horsepower', 'peakrpm', 'citympg', 'highwaympg', 'price']\n","filtered_dt = df[filtered_columns]\n","\n","scaler = StandardScaler()\n","scaler.fit(filtered_dt)\n","sc_filtered_dt = scaler.transform(filtered_dt)\n","\n","X = sc_filtered_dt[:, :-1]\n","y = sc_filtered_dt[:, -1]"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-IS7FaNLHeCM"},"source":["# Step 3: Training the model and performance evaluation: Regression model to estimate the price of a car based on vehicles's features."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"7WzkVlRjIXrN"},"source":["**[EX7]** Build some **utils** functions we will need for our MLP architecture. Create:\n","- *sigmoid* function that calculates the sigmoid of a value, array, etc....\n","- *sigmoid_derivative* funtion that calculates the derivative of sigmoid for a value p.\n","- *relu* function that calculates the relu of a value, array, etc....\n","- *relu_derivative* funtion that calculates the derivative of relu for a value x."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def sigmoid(value):\n","    return 1 / (1 + np.exp(-value))\n","\n","def sigmoid_derivative(p):\n","    return p * (1-p)\n","\n","def relu(value):\n","    return max(0, value)\n","\n","def relu_derivative(x):\n","    return (int)(x > 0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"InRGHujBMjTG"},"source":["**[EX8]** Split `x` and `y` into `xtrain` , `xtest` , `ytrain` , `ytest` with 20% of total samples for testing usage."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"D-FJNphvNEDi"},"source":["**[EX9]** Complete the following code to build of MLP solution with 1 hidden layer. In particular, you should:\n","- 1) complete the **feedforward** method. Select the activation function of the utils section that you consider suitable for this use case (i.e. car price estimation).\n","- 2) complete the **backpropagation** method\n","- 3) build the **predict** method that calculates the output of the MLP based on the last calculated weights during the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xr9U3qsQeqv4"},"outputs":[],"source":["# Class definition\n","class NeuralNetwork:\n","    def __init__(self, x, y):\n","        self.input = x\n","        self.number_neurons_hidden=6\n","        print(self.input.shape[1]+1)\n","        self.weights1= np.random.rand((self.input.shape[1]+1),self.number_neurons_hidden) # considering we have number_neurons_hidden nodes in the hidden layer and include and extra w for w0\n","        print(\"Initialized weights layer1\\n\",self.weights1)\n","        self.weights2 = np.random.rand((self.number_neurons_hidden+1), 1)# considering we have number_neurons_hidden nodes in the hidden layer and include and extra w for w0 for last neuron\n","        print(\"Initialized weights layer2\\n\",self.weights2)\n","        self.y = y\n","        self.output = np.zeros(y.shape)\n","        self.lr=0.01\n","        \n","    def feedforward(self):\n","        #We add a column of \"1\" to input_data to multiply with w0 at first hidden layer\n","        self.input_aux=np.c_[np.ones(self.input.shape[0]), self.input]\n","\n","        #Calculate the output of the first layer\n","        self.layer1_output = sigmoid(self.input_aux @ self.weights1) # layer1\n","        \n","\n","        #We add a column of \"1\" to input data to the second layer to multiply with w0 at second layer\n","        self.layer1_output_aux=np.c_[np.ones(self.layer1_output.shape[0]),self.layer1_output]\n","        \n","        #Calculate the output of the first layer\n","        self.layer2_output = sigmoid(self.layer1_output_aux @ self.weights2) # layer2\n","        \n","\n","        self.output=self.layer2_output\n","        return self.output\n","        \n","    def backpropagation(self):\n","        m = self.input.shape[0]\n","        #Calculate the gradient of the Error vs the output\n","        gradient_output = (self.output - self.y)\n","        \n","        #Calculate the gradient of the Error vs weigths at layer 2\n","        gradient_weights2 = gradient_output * sigmoid_derivative(self.output)\n","\n","        #Calculate the gradient of the Error vs weigths at layer 1\n","        gradient_weights1 = gradient_weights2 @ self.weights2.T * sigmoid_derivative(self.layer1_output_aux)\n","        # gradient_output @ sigmoid_derivative(self.layer1_output_aux @ self.weights2).T @ sigmoid_derivative(self.input_aux @ self.weights1) @ self.input_aux.T\n","        \n","        #Update the weights1 and weights2 according to gradient_weights1 and gradient_weights2 previously calculated and the learning_rate defined at the constructor of the NeuralNetwork class\n","        self.weights1 = self.weights1 - self.lr * gradient_weights1 / m\n","        self.weights2 = self.weights2 - self.lr * gradient_weights2 / m\n","          \n","\n","    def train(self, X, y):\n","        self.output = self.feedforward()\n","        self.backpropagation()\n","\n","    def predict(self, X_pred):\n","    \n","      self.input_predict=X_pred\n","      #Calculate the output of the MLP using the chosen activation function and the weights at layers 1 and 2 once fitted. \n","      \n","      return self.output\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"4eQ9o3_KSTVq"},"source":["We are ready to train our model. Execute the following code to train our MLP based on xtrain and ytrain. Tip: Maybe ytrain should be reshaped to be adjusted to be an array."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":638,"status":"ok","timestamp":1686543053323,"user":{"displayName":"MIGUEL ANGEL CORDOBES ARANDA","userId":"03161467566781317935"},"user_tz":-120},"id":"ujKJaLOgeqv_","outputId":"f1f5aab4-4e2e-45fb-cda1-f0e97e463fe6"},"outputs":[],"source":["#Vector to store the loss in every iteration\n","loss=[]\n","#Create the neural network\n","NN = NeuralNetwork(X_train, y_train.reshape(-1,1))\n","#Number of iterations\n","epochs=1500\n","for i in range(epochs):\n","    NN.train(X_train, y_train)\n","    loss.append(np.mean(np.square(y - NN.feedforward())))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"G1NuxC-AzQqS"},"source":["Use the following **summarize_diagnostics** function to plot the loss."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TZBFVSDAvxp6"},"outputs":[],"source":["def summarize_diagnostics(history):\n","    # plot loss\n","    pyplot.subplot(211)\n","    pyplot.title('Loss')\n","    pyplot.plot(history, color='blue', label='train')\n","    pyplot.legend(loc=\"lower center\", fontsize=14)\n","    return"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ULF18VQdza8o"},"source":["**[EX10]** Plot the **loss** obtained during the training stage of the MLP. Describe the visualization"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"yAm6vnSu0WIl"},"source":["**[EX11]** Execute the **predict** method of the NeuralNetwork class for xtest. Answer the following questions:\n","- (1) Which is the performance of the model using ytest. Is it a good model? Justify your answer.\n","- (2) Which is the mean absolute error at the original scale of the dataset (i.e. previously to normalization)?\n","- (3) In case of a not too good performance, how would you tune the MLP solution (i.e. which parameters of the NN would you modify)? Execute the training and predict process again with this new setup and compare with the previous results. Are the results (i.e. R2 score and mean absolute error) as you expected? Justify your answer.\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.3"}},"nbformat":4,"nbformat_minor":0}
